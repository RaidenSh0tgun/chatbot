# -*- coding: utf-8 -*-
"""chatbot_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tXsADFhH5YhWzbO1TL_dk-WOMPNh3zvf
"""

# Install pip package manager.
!pip -qqq install pip --progress-bar off > /dev/null 2>&1
# Install a package necessary for qdrant, which is for vector database interaction.
!pip -qqq install onnxruntime==1.19.2 --progress-bar off > /dev/null 2>&1
# Install LangChain Groq integration library version 0.1.3.
!pip -qqq install langchain-groq==0.1.3 --progress-bar off > /dev/null 2>&1
# Install LangChain framework version 0.1.17.
!pip -qqq install langchain==0.1.17 --progress-bar off > /dev/null 2>&1
# Install LlamaParse library version 0.1.3 for document parsing.
!pip -qqq install llama-parse==0.1.3 --progress-bar off > /dev/null 2>&1
# Install Qdrant client library version 1.9.1 for vector database interaction.
!pip -qqq install qdrant-client==1.9.1  --progress-bar off > /dev/null 2>&1
# Install Unstructured library version 0.13.6 for handling Markdown files.
!pip -qqq install "unstructured[md]"==0.13.6 --progress-bar off > /dev/null 2>&1
# Install FastEmbed library version 0.2.7 for embedding generation.
!pip -qqq install fastembed==0.2.7 --progress-bar off > /dev/null 2>&1
# Installing the flashrank package version 0.2.4 for efficient information retrieval
!pip -qqq install flashrank==0.2.4 --progress-bar off > /dev/null 2>&1

# Importing the os module to interact with the operating system
import os

# Importing the textwrap module for text formatting functions
import textwrap

# Importing the Path class from pathlib to handle file paths
from pathlib import Path

# Importing the userdata module from Google Colab for user data interactions
from google.colab import userdata

# Importing the Markdown class from IPython.display to display markdown content in Jupyter notebooks
from IPython.display import Markdown

# Importing the RetrievalQA chain from LangChain for question-answering capabilities
from langchain.chains import RetrievalQA

# Importing the PromptTemplate class from LangChain for creating prompt templates
from langchain.prompts import PromptTemplate

# Importing the ContextualCompressionRetriever from LangChain for contextual information retrieval
from langchain.retrievers import ContextualCompressionRetriever

# Importing the FlashrankRerank class from LangChain for document compression and ranking
from langchain.retrievers.document_compressors import FlashrankRerank

# Importing the RecursiveCharacterTextSplitter from LangChain for splitting text recursively by characters
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Importing the Qdrant class from LangChain for vector store operations
from langchain.vectorstores import Qdrant

# Importing the UnstructuredMarkdownLoader from LangChain Community for loading markdown documents
from langchain_community.document_loaders import UnstructuredMarkdownLoader

# Importing the FastEmbedEmbeddings from LangChain Community for fast embeddings generation
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings

# Importing the ChatPromptTemplate from LangChain Core for creating chat prompt templates
from langchain_core.prompts import ChatPromptTemplate

# Importing the ChatGroq class from LangChain Groq for chat functionalities with Groq
from langchain_groq import ChatGroq

# Importing the LlamaParse class from llama_parse for parsing documents
from llama_parse import LlamaParse

# Importing the getpass module for securely handling the API key prompts
import getpass

# Suppress warnings (for better experience during this session; not recommended in future)
import warnings
warnings.filterwarnings("ignore")

import os
from groq import Groq
from flask import Flask, request, jsonify
import textwrap

os.environ["GROQ_API_KEY"] = "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"

# Initialize Groq client
client = Groq(api_key=os.environ["GROQ_API_KEY"])

# Initialize Flask app
app = Flask(__name__)

def process_user_input(user_input):
    # Create chat completion
    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": user_input,
            }
        ],
        model="llama3-70b-8192",
    )
    return chat_completion

def format_response(response):
    # Format the response for better readability
    response_txt = response.choices[0].message.content
    formatted_response = []
    for chunk in response_txt.split("\n"):
        if not chunk:
            formatted_response.append("")
            continue
        formatted_response.append("\n".join(textwrap.wrap(chunk, 100, break_long_words=False)))
    return "\n".join(formatted_response)

@app.route('/chat', methods=['POST'])
def chat():
    # Get user input from POST request
    user_input = request.json.get("message")
    if not user_input:
        return jsonify({"error": "Message is required"}), 400

    # Process the input and get response
    try:
        response = process_user_input(user_input)
        formatted_response = format_response(response)
        return jsonify({"response": formatted_response})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)